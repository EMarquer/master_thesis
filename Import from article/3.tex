\section{Training Setup}\label{seq:training}
In this section we present our training process and dataset.
We introduce a data augmentation process to improve the generalization capability of the model.

We rely on plain random generation for the formal contexts, and not on more involved generation processes as discussed in~\cite{random-closure:2011:ganter,random-context-dirichlet:2019:felde}.
Our generation process introduces biases, and data augmentation is a simple way to compensate some of them.
Indeed, it mimics some of the properties found in real-world datasets which may be under represented in the random contexts.
%
The exact process is described in \autoref{sec:augment}.

\subsection{Training Process}
We train BoA in two phases of 5000 epochs each.
In the first phase, we apply the reconstruction loss and the KL divergence only.
Then, we gradually introduce the co-intent similarity and number of concepts predictions.

When using metric learning methods with multiple metrics, a common approach is to split the embedding space and to learn one metric per sub-part of the embedding space~\cite{deep-metric-multispeaker:2020:kulkarni}.
We use the same principle and use 50\% of the embedding space to predict the co-intent similarity and 25\% for the number of concepts.

The exact embedding dimension of BoA is 128, with a pre-embedding of size of 64.
The LSTM and the bi-LSTM have two layers each.
The decoder MLP has four layers, and the MLPs used to predict our pseudo metrics both have two layers.
We use a ReLU activation function between all the layers of our model.


\subsection{Training Data}
The dataset used for training the BoA model is composed of 6000 randomly generated formal contexts split into training and validation, and the corresponding lattices computed using the Coron system\footnote{\url{http://coron.loria.fr/site/index.php}}.%\todo{Find if there is a bibliographic reference for Coron (nothing found on the website sadly)}

To generate a context of $|O|$ objects and $|A|$ attributes we sample $|O|\times|A|$ values from a Poisson law and apply a threshold of $0.3$. Values under the threshold correspond to $1$ in the context.
Note that the random generation process may result in empty rows and columns, which will be dropped if they are at the extremities of the context.\todo[color=green!40]{Note that the random generation process may result in empty rows and columns. Those at the extremities of the context will be ignored, due to the BASENUM file format used by Coron.}
For this reason, the actual size of the generated context may be smaller than the requested one.
%Also, random contexts generated by our setup tends to produce a larger amount of concepts than natural contexts of the same size.

We generate a training set of 5000 contexts and a test set of 1000 samples. For the training phase, a development set of 10\% of the training set is randomly sampled from the training set. 
For each set, we generate different sizes of contexts, 20\% of each: $5 \times 5$, $10 \times 10$, $10 \times 20$, $20 \times 10$ and $20 \times 20$ contexts ($|O|\times|A|$).
Descriptive statistics on the datasets are reported in \autoref{tab:rand-dataset}.

\begin{table}[t]
\caption{Descriptive statistics on the dataset of randomly generated contexts.}\label{tab:rand-dataset}
\centering
\begin{tabularx}{.9\textwidth}{ll>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X}
\toprule
 && Object number & Attribute number & Concept number & Context density \\
\midrule
\multirow{2}{3.5em}{mean $\pm$~std.}  
& train set & $12.83 \pm 6.11$ & $12.98 \pm 6.03$ & $77.93 \pm 78.39$ & $0.329 \pm 0.057$ \\
& test set  & $12.83 \pm 6.13$ & $12.97 \pm 6.04$ & $78.12 \pm 77.27$ & $0.332 \pm 0.057$ \\
\hline
\multirow{2}{*}{min}
& train set & 1 & 2 & 1 & 0 \\
& test  set & 2 & 3 & 2 & 0 \\
\hline
\multirow{2}{*}{max}  
& train set & 20 & 20 & 401 & 0.56 \\
& test  set & 20 & 20 & 376 & 0.49 \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Data Augmentation}\label{sec:augment}
Our data augmentation is the following pipeline: \textit{(i)} duplicating of objects and attributes, \textit{(ii)} inverting the value of cells and \textit{(iii)} shuffling objects and attributes.
With this process, we simulate identical (duplication) and nearly identical (duplication + drop) objects or attributes that appear in real-world datasets.

Objects and attributes have a probability $p$ of being duplicated.
If duplicated, they have the same probability $p$ of being duplicated again.
From this definition, the number of copies of an object (or attribute) follow a geometric law with a probability of success $p$.
Consequently, the exact number of objects and attributes actually seen during training do not match the ones reported in \autoref{tab:rand-dataset}.
Nonetheless, as the duplication follows a geometric law, we can estimate the number amount of object and attribute seen (see \autoref{equ:estimate_size}).
Inverting some randomly selected values in the formal context is our adaptation of dropout, a common technique in deep learning.
%Shuffling after duplication avoid that the model rely on the order of objects and attributes.\todo{A bit confusing}
The shuflling after duplication avoids model's reliance on oreder of the objects and the attributes.

When co-intent similarity is used, duplication and shuffling are reproduced on the intents.
However, drops in a formal context alter the correspond lattice, so they are not applied at all when using data from the lattice.
This precaution avoids making the model insensitive to small variations in the input.

We set the duplication probability to $p=0.1$ and the drop probability to $0.01$.
In this setting, the estimated average object and attribute numbers are respectively $14.25$ and $14.42$, for both the training and development sets.

\begin{equation}
    \text{Expected size}(size, p_\text{duplication}) = \frac{1}{1 - p_\text{duplication}} \times{} size
    \label{equ:estimate_size}
\end{equation}